{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Congratulations!!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have came up to this point and open ipython notebook, you should have successfully installed the tools required for DL4MT. \n",
    "To test that you can import all the tools that you've just downloaded, click on the next box and then press `shift` + `enter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py, cython, pydot, numpy, scipy, theano, fuel, sklearn, nltk, gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should not see any error when you press `shift` + `enter` on the previous line. \n",
    "\n",
    "In `ipython-notebook`, every \"clickable\" box is refer to as a cell. And you can switch between a text cell, where you can write text like this or a code cell where you can execute python code on the fly (as above).\n",
    "\n",
    "If you're already a fluent Parseltongue (Python) coder, please go ahead with \n",
    "\n",
    " - the [summerschool exercises from MILA](https://github.com/mila-udem/summerschool2015) or\n",
    " - take a look at the nice introduction to [Neural Machine Translation](http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-with-gpus/) from Prof. Kyunghyun Cho or\n",
    " - read more about the [Unreasonable Effectiveness of Recurrent Neural Nets](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) or\n",
    " - enjoy the nice [Neural Net NLP primer](http://u.cs.biu.ac.il/~yogo/nnlp.pdf) from Yoav Goldberg or\n",
    " - hone your snake charming, please do take a look at [Fluent Python](http://shop.oreilly.com/product/0636920032519.do) \n",
    " \n",
    " \n",
    "Pythonic Fun\n",
    "=====\n",
    " \n",
    "The following lines would be just a few common pythonic idioms that you might see in the following days. \n",
    "To proceed, click on each ipython notebook code cell and press `shift` + `enter`...\n",
    "\n",
    "\n",
    "File I/O\n",
    "====\n",
    "\n",
    "To open a text file in Python, it's best to use the [`io` module](https://docs.python.org/2/library/io.html) and the `with` operator, you can also specify the encoding for your file. Note that when using the `with` statement, the file automatically closes when you're out of the with scope. There is no need to explicitly close a file when using `with`.\n",
    "\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io # This is the io module.\n",
    "# This is the with statement that controls the file I/O\n",
    "with io.open('test.txt', 'r', encoding='utf8') as fin: \n",
    "    for line in fin: # Iterates through each line in the file.\n",
    "        print line # prints the line to console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle (not Sauerkraut)\n",
    "----\n",
    "\n",
    "Pickling is a concept to save a python object as binary and then read them again without the hassle of packing and unpacking an object from a specific file format. The [`pickle` object](https://docs.python.org/2/library/pickle.html) is a nifty way to save your trained models and load them in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import cPickle as pickle # Note that in python3, you simple do `import pickle`\n",
    "from collections import Counter # This is a counter object, see https://docs.python.org/2/library/collections.html\n",
    "\n",
    "word_counts = Counter() # Let's initialize a word counter object to keep count of words in a file.\n",
    "with io.open('test.txt', 'r', encoding='utf8') as fin:\n",
    "    text = fin.read() # Reads the whole file as a single string object.\n",
    "    text = text.split() # Let's split the text into a list of words.\n",
    "    word_counter.update(text)\n",
    "    \n",
    "with io.open('wordcounts.pk', 'wb') as fout: # Note that 'wb' means a file for writing objects as binary objects.\n",
    "    pickle.dump(word_counts, fout) # Dumps the *word_counts* object into a pickle file named 'wordcounts.pk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you would see a new file `wordcounts.pk` appear in the directory where this ipython notebook resides. You can easily read a `word_counter` now by loading the pickled object rather than recounting from the textfile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import cPickle as pickle\n",
    "\n",
    "with io.open('wordcounts.pk', 'rb') as fin: # Opens a binary file with the 'rb' parameter/flag.\n",
    "    word_counts = pickle.load(fin)\n",
    "    \n",
    "for word, count in word_counts.items(): # Iterates through the newly loaded *word_counts* object.\n",
    "    print word, count # prints the word and its count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some NLP (atlas)\n",
    "====\n",
    "\n",
    "Let's get down to some NLP work given the knowledge of pickles and file I/O in python. First let's start with some corpus access, Part-of-Speech (POS) and tokenization tools that `NLTK` provides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "from nltk.corpus import brown # This the tagged brown corpus which is also a subset of the Penntreebank corpus.\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag # The default sentence, word tokenizer and pos-tagger from NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# TODO: Please fill in the code to read the `test.txt` file.\n",
    "# (remember to press `shift` + `enter` after the code)\n",
    "#############################################################\n",
    "\n",
    "with io.open('test.txt', 'r', encoding='utf8') as fin:\n",
    "    text = fin.read()\n",
    "    print text\n",
    "\n",
    "    \n",
    "###########################\n",
    "# Answer: DON'T PEEK!!!!\n",
    "###########################\n",
    "#with io.open('test.txt', 'r', encoding='utf8') as fin:\n",
    "#    text = fin.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have read the file, let's try to tokenize the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sentence in sent_tokenize(text):\n",
    "    for word in word_tokenize(sentence):\n",
    "        print word\n",
    "    print '# END of sentence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sentence in sent_tokenize(text):\n",
    "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
    "        print word, tag\n",
    "    print '########'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To get the tagged words into a pickle-able object, \n",
    "# you can keep the whole process tagged file as a \n",
    "# list of list of strings using list comprehension, \n",
    "# see https://docs.python.org/2/tutorial/datastructures.html#list-comprehensions\n",
    "\n",
    "tagged_text = [pos_tag(word_tokenize(sentence)) for sentence in sent_tokenize(text)]\n",
    "print tagged_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# Now try to pickle the *tagged_text* into a pickle file,\n",
    "# and then try to open the newly pickled file.\n",
    "##########################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################\n",
    "# Answer: DON'T PEEK!!!!\n",
    "###########################\n",
    "\n",
    "#import pickle\n",
    "#with io.open('tagged_text.pk', 'wb') as fout:\n",
    "#    tagged_text = [pos_tag(word_tokenize(sentence)) for sentence in sent_tokenize(text)]\n",
    "#    pickle.dump(tagged_text, fout)\n",
    "#    \n",
    "#with io.open('tagged_text.pk', 'rb') as fin:\n",
    "#    pickled_tagged_text = pickle.load(fin)\n",
    "#    \n",
    "#print pickled_tagged_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some More NLP\n",
    "====\n",
    "\n",
    "Alright, that's how NLTK tokenizes and POS tag a corpus. Now, let's skip all the hard work of tagging a corpus and just simply read one off NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "tagged_text = brown.tagged_sents()\n",
    "print tagged_text # Note: this is the same data structure as the pickled tagged_text above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
