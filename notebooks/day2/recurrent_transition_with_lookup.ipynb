{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pylab\n",
    "import sys\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import logging\n",
    "import pprint\n",
    "import math\n",
    "import numpy\n",
    "import numpy as np\n",
    "import os\n",
    "import operator\n",
    "import theano\n",
    "\n",
    "from theano import tensor\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from theano import function\n",
    "from fuel.datasets import IndexableDataset\n",
    "from fuel.transformers import Mapping, Batch, Padding, Filter\n",
    "from fuel.schemes import (ConstantScheme, ShuffledScheme,\n",
    "                          ShuffledExampleScheme, SequentialExampleScheme,\n",
    "                          BatchSizeScheme)\n",
    "from fuel.transformers import Flatten\n",
    "from fuel.streams import DataStream\n",
    "from blocks.config import config\n",
    "from blocks.bricks import Tanh, Initializable, Logistic, Identity\n",
    "from blocks.bricks.base import application\n",
    "from blocks.bricks import Linear, Rectifier, Softmax\n",
    "from blocks.graph import ComputationGraph\n",
    "from blocks.bricks.lookup import LookupTable\n",
    "from blocks.bricks.recurrent import SimpleRecurrent, GatedRecurrent, LSTM, Bidirectional\n",
    "from blocks.bricks.parallel import Fork\n",
    "from blocks.bricks.sequence_generators import (\n",
    "    SequenceGenerator, Readout, SoftmaxEmitter, LookupFeedback)\n",
    "from blocks.algorithms import (GradientDescent, Scale,\n",
    "                               StepClipping, CompositeRule, AdaDelta, Adam)\n",
    "from blocks.initialization import Orthogonal, IsotropicGaussian, Constant\n",
    "from blocks.bricks.cost import CategoricalCrossEntropy\n",
    "from blocks.bricks.cost import SquaredError\n",
    "from blocks.serialization import load_parameter_values\n",
    "from blocks.model import Model\n",
    "from blocks.monitoring import aggregation\n",
    "from blocks.extensions import FinishAfter, Printing, Timing\n",
    "from blocks.extensions.saveload import Checkpoint\n",
    "from blocks.extensions.monitoring import TrainingDataMonitoring\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.bricks import WEIGHT\n",
    "from blocks.roles import INPUT\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.graph import apply_dropout\n",
    "from blocks.utils import named_copy, dict_union, shared_floatx_nans, shared_floatx_zeros, shared_floatx\n",
    "from blocks.utils import dict_union, shared_floatx_nans, shared_floatx_zeros, shared_floatx\n",
    "from blocks.bricks.recurrent import BaseRecurrent\n",
    "from blocks.bricks.wrappers import As2D\n",
    "from blocks.bricks.base import lazy\n",
    "from blocks.bricks.recurrent import recurrent\n",
    "from blocks.roles import add_role, INITIAL_STATE\n",
    "from blocks.extensions import SimpleExtension\n",
    "from blocks.bricks import MLP\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SOME USEFUL STUFF FOR THEANO DEBUGGING\n",
    "\n",
    "config.recursion_limit = 100000\n",
    "floatX = theano.config.floatX\n",
    "logger = logging.getLogger(__name__)\n",
    "# this is to let the log print in the notebook\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# theano debugging stuff\n",
    "theano.config.optimizer='fast_compile'\n",
    "# theano.config.optimizer='None'\n",
    "theano.config.exception_verbosity='high'\n",
    "\n",
    "# compute_test_value is 'off' by default, meaning this feature is inactive\n",
    "# theano.config.compute_test_value = 'off' # Use 'warn' to activate this feature\n",
    "# theano.config.compute_test_value = 'warn' # Use 'warn' to activate this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some filter and mapping functions for fuel to use\n",
    "def _transpose(data):\n",
    "    return tuple(array.T for array in data)\n",
    "\n",
    "# swap the (batch, time) axes to make the shape (time, batch, ...)\n",
    "def _swapaxes(data):\n",
    "    return tuple(array.swapaxes(0,1) for array in data)\n",
    "\n",
    "def _filter_long(data):\n",
    "    return len(data[0]) <= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is how you could load a bigger slice of the corpus\n",
    "# from nltk.corpus import brown\n",
    "# words_by_line, tags_by_line = zip(*[zip(*sen) for sen in list(brown.tagged_sents())[:10000]])\n",
    "# for these examples, we need to make sure every seq is the same length to avoid padding/mask issues\n",
    "\n",
    "# a tiny toy dataset for learning the POS of the ambiguous word \"calls\"\n",
    "words_by_line = [['she', 'calls', 'me', 'every', 'day', '.'],  \n",
    "                 ['I', 'received', 'two', 'calls', 'yesterday', '.']] *10\n",
    "\n",
    "\n",
    "tags_by_line = [[u'PPS', u'VBZ', u'PPO', u'AT', u'NN', u'.'],\n",
    "                [u'PPSS', u'VBN', u'CD', u'NNS', u'NR', u'.']] *10\n",
    "\n",
    "idx2word = dict(enumerate(set([w for l in words_by_line for w in l])))\n",
    "word2idx = {v:k for k,v in idx2word.items()}\n",
    "\n",
    "idx2tag = dict(enumerate(set([t for l in tags_by_line for t in l])))\n",
    "tag2idx = {v:k for k,v in idx2tag.items()}\n",
    "\n",
    "iwords = [[word2idx[w] for w in l] for l in words_by_line]\n",
    "itags =  [[tag2idx[t] for t in l] for l in tags_by_line]\n",
    "\n",
    "# now create the fuel dataset\n",
    "qe_dataset = IndexableDataset(\n",
    "    indexables=OrderedDict([('words', iwords), ('tags', itags)]))\n",
    "\n",
    "# now we're going to progressively wrap data streams with other streams that transform the stream somehow\n",
    "qe_dataset.example_iteration_scheme = ShuffledExampleScheme(qe_dataset.num_examples)\n",
    "data_stream = qe_dataset.get_example_stream()\n",
    "data_stream = Batch(data_stream, iteration_scheme=ConstantScheme(1))\n",
    "\n",
    "# add padding and masks to the dataset\n",
    "# data_stream = Padding(data_stream, mask_sources=('words','tags'))\n",
    "data_stream = Padding(data_stream, mask_sources=('words'))\n",
    "data_stream = Mapping(data_stream, _swapaxes)\n",
    "\n",
    "# Example of how the iterator works\n",
    "# for batch in list(data_stream.get_epoch_iterator()):\n",
    "#     print([source.shape for source in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagset_size=len(tag2idx.keys())\n",
    "vocab_size=len(word2idx.keys())\n",
    "dimension=5\n",
    "\n",
    "class LookupRecurrent(BaseRecurrent, Initializable):\n",
    "    \"\"\"The recurrent transition with lookup and feedback \n",
    "\n",
    "    The most well-known recurrent transition: a matrix multiplication,\n",
    "    optionally followed by a non-linearity.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        The dimension of the hidden state\n",
    "    activation : :class:`.Brick`\n",
    "        The brick to apply as activation.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    See :class:`.Initializable` for initialization parameters.\n",
    "\n",
    "    \"\"\"\n",
    "    @lazy(allocation=['dim'])\n",
    "    def __init__(self, dim, activation, **kwargs):\n",
    "        super(LookupRecurrent, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        \n",
    "        word_lookup = LookupTable(vocab_size, dimension)\n",
    "        word_lookup.weights_init = IsotropicGaussian(0.01)\n",
    "        word_lookup.initialize()\n",
    "        self.word_lookup = word_lookup\n",
    "\n",
    "        # There will be a Softmax on top of this layer\n",
    "        state_to_output = Linear(name='state_to_output', input_dim=dimension, output_dim=tagset_size)\n",
    "        state_to_output.weights_init = IsotropicGaussian(0.01)\n",
    "        state_to_output.biases_init = Constant(0.0)\n",
    "        state_to_output.initialize()\n",
    "        self.state_to_output = state_to_output\n",
    "\n",
    "        # note - As2D won't work with masks\n",
    "        nonlinearity = Softmax()\n",
    "        wrapper_2D = As2D(nonlinearity.apply)\n",
    "        wrapper_2D.initialize()\n",
    "        self.wrapper_2D = wrapper_2D\n",
    "\n",
    "        # the \"lookup\" brick -- aka the word embeddings\n",
    "        lookup = LookupFeedback(num_outputs=tagset_size, feedback_dim=dimension)\n",
    "        lookup.weights_init = IsotropicGaussian(0.1)\n",
    "        lookup.biases_init = Constant(0.0)\n",
    "        lookup.initialize()\n",
    "        self.lookup = lookup\n",
    "        \n",
    "        # a non-linear activation (i.e. Sigmoid, Tanh, ReLU, ...)\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.children = [activation, state_to_output, nonlinearity, \n",
    "                         wrapper_2D, word_lookup, lookup]\n",
    "\n",
    "    @property\n",
    "    def W(self):\n",
    "        return self.parameters[0]\n",
    "\n",
    "    def get_dim(self, name):\n",
    "        if name == 'mask':\n",
    "            return 0\n",
    "        if name in (LookupRecurrent.apply.sequences +\n",
    "                    LookupRecurrent.apply.states):\n",
    "            return self.dim\n",
    "        return super(LookupRecurrent, self).get_dim(name)\n",
    "\n",
    "    # the initial state is the 'original' lookup+feedback\n",
    "    # the initial state is combined with the first input to produce the first output\n",
    "    def _allocate(self):\n",
    "        self.parameters.append(shared_floatx_nans((self.dim, self.dim),\n",
    "                                                  name=\"W\"))\n",
    "        add_role(self.parameters[0], WEIGHT)\n",
    "        \n",
    "        self.parameters.append(shared_floatx(np.random.random(self.dim,), name=\"initial_state\"))\n",
    "        add_role(self.parameters[1], INITIAL_STATE)\n",
    "       \n",
    "    def _initialize(self):\n",
    "        self.weights_init.initialize(self.W, self.rng)\n",
    "    \n",
    "    def get_predictions(self, inputs):\n",
    "        linear_mapping = self.state_to_output.apply(inputs)\n",
    "        readouts = self.wrapper_2D.apply(linear_mapping)\n",
    "        return readouts\n",
    "    \n",
    "    # TODO: change inputs-->states or something more clear\n",
    "    def get_feedback(self, inputs):\n",
    "        linear_mapping = self.state_to_output.apply(inputs)\n",
    "        readouts = self.wrapper_2D.apply(linear_mapping)\n",
    "        predictions = readouts.argmax(axis=1)\n",
    "        return self.lookup.feedback(predictions)\n",
    "    \n",
    "    @recurrent(sequences=['inputs', 'mask'], states=['states'], outputs=['states'], contexts=[])\n",
    "    def apply(self, inputs=None, states=None, mask=None):\n",
    "        \"\"\"Apply the transition.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : :class:`~tensor.TensorVariable`\n",
    "            The 2D inputs, in the shape (batch, features).\n",
    "        states : :class:`~tensor.TensorVariable`\n",
    "            The 2D states, in the shape (batch, features).\n",
    "        mask : :class:`~tensor.TensorVariable`\n",
    "            A 1D binary array in the shape (batch,) which is 1 if\n",
    "            there is data available, 0 if not. Assumed to be 1-s\n",
    "            only if not given.\n",
    "\n",
    "        \"\"\"\n",
    "        # first compute the current representation (_not_ state) via the standard recurrent transition\n",
    "        current_representation = self.word_lookup.apply(inputs) + tensor.dot(states, self.W)\n",
    "        next_states = self.children[0].apply(current_representation)\n",
    "        \n",
    "        if mask:\n",
    "            next_states = (mask[:, None] * next_states +\n",
    "                           (1 - mask[:, None]) * states)\n",
    "\n",
    "        return next_states\n",
    "\n",
    "    # trainable initial state\n",
    "    @application(outputs=apply.states)\n",
    "    def initial_states(self, batch_size, *args, **kwargs):\n",
    "        return tensor.repeat(self.parameters[1][None, :], batch_size, 0)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test applying our transition to a batch, and see what we get back\n",
    "transition = LookupRecurrent(dim=dimension, activation=Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is the cost function that we'll use to train our model\n",
    "def get_cost(words,words_mask,targets):\n",
    "\n",
    "#     comment this out if you are using the GatedRecurrent transition\n",
    "    states = transition.apply(\n",
    "        **dict_union(inputs=words, mask=words_mask, return_initial_states=True))\n",
    "    \n",
    "    output = states[1:]\n",
    "    output_shape = output.shape\n",
    "\n",
    "    dim1 = output_shape[0] * output_shape[1]\n",
    "    dim2 = output_shape[2]\n",
    "  \n",
    "    y_hat = Softmax().apply(\n",
    "        transition.state_to_output.apply(\n",
    "        output.reshape((dim1, dim2))))\n",
    "    \n",
    "    # try the blocks crossentropy\n",
    "    y = targets.flatten()\n",
    "    costs = theano.tensor.nnet.categorical_crossentropy(y_hat,y)\n",
    "    \n",
    "    final_cost = costs.mean()\n",
    "    \n",
    "#     return final_cost\n",
    "    return (final_cost, y_hat, y, costs, final_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_prediction(words, words_mask):\n",
    "    \n",
    "    states = transition.apply(\n",
    "        **dict_union(inputs=words, mask=words_mask, return_initial_states=True))\n",
    "    \n",
    "    # we only care about the RNN states, which are the first and only output\n",
    "    output = states[1:]\n",
    "    output_shape = output.shape\n",
    "    dim1 = output_shape[0] * output_shape[1]\n",
    "    dim2 = output_shape[2]\n",
    "    \n",
    "    y_hat = Softmax().apply(\n",
    "        transition.state_to_output.apply(\n",
    "            output.reshape((dim1, dim2))))\n",
    "\n",
    "    predictions = y_hat\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/programs/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:135: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n",
      "  from scan_perform.scan_perform import *\n"
     ]
    }
   ],
   "source": [
    "words=tensor.lmatrix(\"words\")\n",
    "words_mask=tensor.matrix(\"words_mask\")\n",
    "targets=tensor.lmatrix(\"tags\")\n",
    "# targets_mask=tensor.matrix(\"tags_mask\")\n",
    "\n",
    "# let's get some feedback from the cost function so we can monitor it\n",
    "cost, yhat, y, raw_costs, true_costs = get_cost(words, words_mask, targets)\n",
    "\n",
    "yhat.name = 'yyhat'\n",
    "y.name = 'y_inside'\n",
    "raw_costs.name = 'raw_costs'\n",
    "true_costs.name = 'true_costs'\n",
    "# mlp_cost.name = 'mlp_cost'\n",
    "\n",
    " \n",
    "# can we just get ther computation graph directly here? (without Model)\n",
    "cost_cg = ComputationGraph(cost)\n",
    "weights = VariableFilter(roles=[WEIGHT])(cost_cg.variables)\n",
    "\n",
    "cost.name = \"sequence_log_likelihood_cost_regularized\"\n",
    "prediction_model = Model(get_prediction(words, words_mask)).get_theano_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Parameters:\n",
      "[('/lookuprecurrent/state_to_output.b', (11,)),\n",
      " ('/lookuprecurrent.initial_state', (5,)),\n",
      " ('/lookuprecurrent/lookuptable.W', (10, 5)),\n",
      " ('/lookuprecurrent.W', (5, 5)),\n",
      " ('/lookuprecurrent/state_to_output.W', (5, 11))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------\n",
      "BEFORE FIRST EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 0\n",
      "\t received_first_batch: False\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 0:\n",
      "\t time_initialization: 0.361103773117\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 49\n",
      "\t iterations_done: 1000\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 1000:\n",
      "\t average_sequence_log_likelihood_cost_regularized: 2.30121779442\n",
      "\t sequence_log_likelihood_cost_regularized: 2.13266158104\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 99\n",
      "\t iterations_done: 2000\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 2000:\n",
      "\t average_sequence_log_likelihood_cost_regularized: 1.82736539841\n",
      "\t sequence_log_likelihood_cost_regularized: 1.3605761528\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 149\n",
      "\t iterations_done: 3000\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 3000:\n",
      "\t average_sequence_log_likelihood_cost_regularized: 1.14914631844\n",
      "\t sequence_log_likelihood_cost_regularized: 0.801165759563\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 199\n",
      "\t iterations_done: 4000\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 4000:\n",
      "\t average_sequence_log_likelihood_cost_regularized: 0.692778944969\n",
      "\t sequence_log_likelihood_cost_regularized: 0.567239224911\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 249\n",
      "\t iterations_done: 5000\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 5000:\n",
      "\t average_sequence_log_likelihood_cost_regularized: 0.429486840963\n",
      "\t sequence_log_likelihood_cost_regularized: 0.340382009745\n",
      "\t training_finish_requested: True\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "TRAINING HAS BEEN FINISHED:\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 249\n",
      "\t iterations_done: 5000\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 5000:\n",
      "\t average_sequence_log_likelihood_cost_regularized: 0.429486840963\n",
      "\t sequence_log_likelihood_cost_regularized: 0.340382009745\n",
      "\t training_finish_requested: True\n",
      "\t training_finished: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct the main loop and start training!\n",
    "\n",
    "transition.weights_init = IsotropicGaussian(0.1)\n",
    "transition.biases_init = Constant(0.)\n",
    "transition.initialize()\n",
    "\n",
    "# Go through this many batches\n",
    "num_batches=5000\n",
    "\n",
    "from blocks.monitoring import aggregation\n",
    "\n",
    "batch_cost = cost\n",
    "final_cost = aggregation.mean(batch_cost, 1)\n",
    "final_cost.name = 'final_cost'\n",
    "test_model = Model(final_cost)\n",
    "\n",
    "cg = ComputationGraph(final_cost)\n",
    "\n",
    "# note that you must explicitly provide the cost function `cost=...`\n",
    "algorithm = GradientDescent(\n",
    "    cost=final_cost, parameters=cg.parameters,\n",
    "    step_rule=CompositeRule([StepClipping(10.0), Scale(0.01)])) \n",
    "\n",
    "#     CompositeRule([StepClipping(10.0), Scale(0.01)]))   \n",
    "\n",
    "#     CompositeRule([StepClipping(10.0), Adam()])   \n",
    "#     step_rule=AdaDelta())\n",
    "# step_rule=Scale(learning_rate=1e-3)\n",
    "#     step_rule=AdaDelta())    \n",
    "#     step_rule=CompositeRule([StepClipping(10.0), Scale(0.01)]))\n",
    "\n",
    "parameters = test_model.get_parameter_dict()\n",
    "logger.info(\"Parameters:\\n\" +\n",
    "                pprint.pformat(\n",
    "                    [(key, value.get_value().shape) for key, value in parameters.items()],\n",
    "                    width=120))\n",
    "\n",
    "observables = [cost]\n",
    "\n",
    "# Some other things you could observe during training\n",
    "#  algorithm.total_step_norm, algorithm.total_gradient_norm]\n",
    "\n",
    "# for name, parameter in parameters.items():\n",
    "#     observables.append(named_copy(\n",
    "#         parameter.norm(2), name + \"_norm\"))\n",
    "#     observables.append(named_copy(\n",
    "#         algorithm.gradients[parameter].norm(2), name + \"_grad_norm\"))\n",
    "\n",
    "# this will be the prefix of the saved model and log\n",
    "save_path='test-lookup-recurrent-model'\n",
    "\n",
    "average_monitoring = TrainingDataMonitoring(\n",
    "    observables, prefix=\"average\", every_n_batches=1000)\n",
    "\n",
    "main_loop = MainLoop(\n",
    "    model=test_model,\n",
    "    data_stream=data_stream,\n",
    "    algorithm=algorithm,\n",
    "    extensions=[\n",
    "        Timing(),\n",
    "        TrainingDataMonitoring(observables, after_batch=True),\n",
    "        average_monitoring,\n",
    "        FinishAfter(after_n_batches=num_batches),\n",
    "        # This is a hook to handle NaN emerging during training -- finish if you see it\n",
    "#         .add_condition([\"after_batch\"], _is_nan),\n",
    "        # hook to save the model \n",
    "#         Checkpoint(save_path, every_n_batches=1000,\n",
    "#                    save_separately=[\"model\", \"log\"]),\n",
    "        Printing(every_n_batches=1000, after_epoch=False)])\n",
    "main_loop.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'PPS', u'VBN', u'CD', u'NNS']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually test some examples to get an idea what your model learned\n",
    "\n",
    "# new_ex = [['she', 'calls', 'me', 'every', 'day']]\n",
    "new_ex = [['she', 'received', 'two', 'calls']]\n",
    "new_ex_int = [[word2idx[w] for w in l] for l in new_ex]\n",
    "example = np.array(new_ex_int).swapaxes(0,1)\n",
    "o = prediction_model(example, np.ones(example.shape).astype(theano.config.floatX))[0]\n",
    "predictions = [idx2tag[i] for i in o.argmax(axis=1)]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges:\n",
    "\n",
    "(1)\n",
    "How many batches do you need before the model learns something useful? Any ideas on how to make the training faster?\n",
    "\n",
    "(2) \n",
    "Play with the hyperparameters, embedding size, recurrent transition size, etc... -- how does this affect the model's performance?\n",
    "\n",
    "(2)\n",
    "- come up with your own small training set of ambiguous examples (maybe in your native language)\n",
    "- do a qualitative analysis of what the recurrent model learns\n",
    "- Note: for this notebook, you need to make sure that your examples are the same length, because we haven't\n",
    "    because we haven't implemented masking \n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
